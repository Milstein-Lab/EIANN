{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb44cf31",
   "metadata": {},
   "source": [
    "# EIANN Tutorial 1:  \n",
    "# Feedforward ANN trained with backprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc94c787",
   "metadata": {
    "tags": [
     "collapse-input",
     "hide-output"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ag1880/github-repos/Milstein-Lab/EIANN/EIANN/utils/data_utils.py:7: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import EIANN.EIANN as eiann"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c41e2ec",
   "metadata": {},
   "source": [
    "## Build and train a simple feedforward neural network with EIANN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1dd2b1",
   "metadata": {},
   "source": [
    "To build a neural network using the EIANN library, you need to specify 3 things:\n",
    "1. The number of neurons in each layer and sub-population\n",
    "2. The projections (how the populations connect to each other)\n",
    "3. Global parameters that we want to apply to the whole network (e.g. learning rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20c5c9d",
   "metadata": {},
   "source": [
    "The EIANN interface is designed to have all of these things specified through python dicts. A network with the specified architecture will then be built using pytorch layers as the backend building blocks. \n",
    "\n",
    "There are two easy ways to create the python dicts that will specify the network architecture:\n",
    "\n",
    "1. Programmatically using the NetworkBuilder class\n",
    "2. Using a .yaml configuration file (you can find examples of these in the network_config directory)\n",
    "\n",
    "The first method is convenient for quickly prototyping and testing different network architectures.  The second method is particularly useful for hyperparameter optimization and experiment reproducibility.\n",
    "\n",
    "Here we will show you a simple example with both methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e36f1d",
   "metadata": {},
   "source": [
    "### Programming interface"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5cfb899",
   "metadata": {},
   "source": [
    "In EIANN, network architecture is defined by: 1. creating populations of neurons, and 2. specifying how they are connected. This approach is flexible and allows you to recurrently connect any two populations, regardless of layer hierarchy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3164fe4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Network Architecture:\n",
      "Input.E (784) -> H1.E (500): No learning rule\n",
      "H1.E (500) -> Output.E (10): No learning rule\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "network_config = eiann.NetworkBuilder()\n",
    "\n",
    "# Define layers and populations\n",
    "network_config.layer('Input').population('E', size=784)\n",
    "network_config.layer('H1').population('E', size=500, activation='relu')\n",
    "network_config.layer('Output').population('E', size=10, activation='softmax')\n",
    "\n",
    "# Create connections between populations\n",
    "network_config.connect(source='Input.E', target='H1.E')\n",
    "network_config.connect(source='H1.E', target='Output.E')\n",
    "\n",
    "# Build the network\n",
    "network_config.print_architecture()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a84e57",
   "metadata": {},
   "source": [
    "Rather than defining only the weight matrix dimensions, this population-based syntax allows you to also specify a number of attributes for each population. For example, if you want the network to respect Dale's Law, you can specify whether each population is excitatory or inhibitory. Doing this will automatically bound the sign of outgoing weights, both at initialization and during training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f456ef6",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'LayerBuilder' object has no attribute 'type'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mnetwork_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mH1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpopulation\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mE\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrelu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtype\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExc\u001b[39m\u001b[38;5;124m'\u001b[39m) \n\u001b[1;32m      2\u001b[0m network_config\u001b[38;5;241m.\u001b[39mlayer(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mH1\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mpopulation(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSomaI\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m50\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mtype(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInh\u001b[39m\u001b[38;5;124m'\u001b[39m) \n",
      "\u001b[0;31mAttributeError\u001b[0m: 'LayerBuilder' object has no attribute 'type'"
     ]
    }
   ],
   "source": [
    "network_config.layer('H1').population('E', 500, 'relu').type('Exc') \n",
    "network_config.layer('H1').population('SomaI', 50, 'relu').type('Inh') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a04fc0",
   "metadata": {},
   "source": [
    "Should you prefer, these weigtht bounds can also be set for each individual projection (weight matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "358d6a76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<EIANN.EIANN._network.ProjectionBuilder at 0x181f44910>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network_config.connect(source='H1.E', target='H1.SomaI').type('Exc') \n",
    "network_config.connect(source='H1.SomaI', target='H1.E').type('Inh') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db2c923",
   "metadata": {},
   "source": [
    "Once we have specified all the populations and projections that define the network architecture, we can use the `build_network` function to create the neural network (pytorch) object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d26523",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eiann",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
