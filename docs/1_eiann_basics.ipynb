{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb44cf31",
   "metadata": {},
   "source": [
    "# Intro to EIANN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b930140",
   "metadata": {},
   "source": [
    "EIANN is a PyTorch-based library for building neural networks. \n",
    "\n",
    "At its core, the syntax for building these networks uses python dicts to specify the network architecture and parameters. We provide two easy ways to generate these dicts: YAML files or a programmatic interface with a NetworkBuilder class. The first method is convenient for quickly prototyping and testing different network architectures. The second method is particularly useful for hyperparameter optimization, systematic architecture search, and experiment reproducibility.\n",
    "Here we will show you examples of how to do both. \n",
    "\n",
    "Since thie library is based on [PyTorch](https://pytorch.org), a basic familiarity with PyTorch objects and syntax is helpful, but not required, to use EIANN. You can geet a quick overview of PyTorch by going through the [PyTorch Quickstart Tutorial](https://pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html).\n",
    "\n",
    "### Basic building blocks\n",
    "To build a neural network using the EIANN library, you need to specify 3 things:\n",
    "1. The number neural populations within in each \"layer\" and how many neurons are in each population\n",
    "2. The projections (how the populations connect to each other)\n",
    "3. Global parameters that we want to apply to the whole network (e.g. learning rate)\n",
    "\n",
    "This population-centric approach is flexible and allows you to recurrently connect any two populations, regardless of layer hierarchy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130ffdd1",
   "metadata": {},
   "source": [
    "Each of these things will be defined through 3 separate python dicts. EIANN will then build a PyTorch network the specified architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e36f1d",
   "metadata": {},
   "source": [
    "### 1. Programming interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "128ab200",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ag1880/github-repos/Milstein-Lab/EIANN/EIANN/utils/data_utils.py:7: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import EIANN.EIANN as eiann"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3164fe4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Network Architecture:\n",
      "Input.E (784) -> H1.E (500): No learning rule\n",
      "H1.E (500) -> Output.E (10): No learning rule\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "network_config = eiann.NetworkBuilder()\n",
    "\n",
    "# Define layers and populations\n",
    "network_config.layer('Input').population('E', size=784)\n",
    "network_config.layer('H1').population('E', size=500, activation='relu')\n",
    "network_config.layer('Output').population('E', size=10, activation='softmax')\n",
    "\n",
    "# Create connections between populations\n",
    "network_config.connect(source='Input.E', target='H1.E')\n",
    "network_config.connect(source='H1.E', target='Output.E')\n",
    "\n",
    "# Build the network\n",
    "network_config.print_architecture()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a84e57",
   "metadata": {},
   "source": [
    "Rather than defining only the weight matrix dimensions, this population-based syntax allows you to also specify attributes at the population level. For example, if you want the network to respect Dale's Law, you can specify whether each population is excitatory or inhibitory. Doing this will automatically bound the sign of outgoing weights, both at initialization and during training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f456ef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Network Architecture:\n",
      "Input.E (784) -> H1.E (500): No learning rule\n",
      "H1.E (500) -> Output.E (10) [Exc]: No learning rule\n",
      "\n",
      "Unconnected populations:\n",
      "H1.SomaI (50) [Inh]\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "network_config.layer('H1').population('E', 500, 'relu').type('Exc') \n",
    "network_config.layer('H1').population('SomaI', 50, 'relu').type('Inh') \n",
    "network_config.print_architecture()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a04fc0",
   "metadata": {},
   "source": [
    "Should you prefer, these weight bounds can also be set or overwritten for individual projections:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "358d6a76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Network Architecture:\n",
      "Input.E (784) -> H1.E (500): No learning rule\n",
      "H1.SomaI (50) -> H1.E (500) [Inh]: No learning rule\n",
      "H1.E (500) -> H1.SomaI (50) [Exc]: No learning rule\n",
      "H1.E (500) -> Output.E (10) [Exc]: No learning rule\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "network_config.connect(source='H1.E', target='H1.SomaI').type('Exc') \n",
    "network_config.connect(source='H1.SomaI', target='H1.E').type('Inh') \n",
    "network_config.print_architecture()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d02962",
   "metadata": {},
   "source": [
    "So far we have not specified the type of learning rule we want to use. If we want to use a single learning rule (e.g. Backprop) throughout the whole network, this can be done globally:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f6ba297",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Network Architecture:\n",
      "Input.E (784) -> H1.E (500): Backprop\n",
      "H1.SomaI (50) -> H1.E (500) [Inh]: Backprop\n",
      "H1.E (500) -> H1.SomaI (50) [Exc]: Backprop\n",
      "H1.E (500) -> Output.E (10) [Exc]: Backprop\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Set Backprop as the learning rule for the whole network\n",
    "network_config.set_learning_rule('Backprop')\n",
    "network_config.print_architecture()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c26a537",
   "metadata": {},
   "source": [
    "If we instead want a different learning rule for a subset of the connections, we can also manually specify it for a given layer, population, or projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d178637c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Network Architecture:\n",
      "Input.E (784) -> H1.E (500): Backprop\n",
      "H1.SomaI (50) -> H1.E (500) [Inh]: BCM\n",
      "H1.E (500) -> H1.SomaI (50) [Exc]: Backprop\n",
      "H1.E (500) -> Output.E (10) [Exc]: Backprop\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "network_config.connect(source='H1.SomaI', target='H1.E').type('Inh').learning_rule('BCM')\n",
    "network_config.print_architecture()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9705fe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Network Architecture:\n",
      "Input.E (784) -> H1.E (500): Hebbian (lr=0.01)\n",
      "H1.SomaI (50) -> H1.E (500) [Inh]: Hebbian (lr=0.01)\n",
      "H1.E (500) -> H1.SomaI (50) [Exc]: Backprop\n",
      "H1.E (500) -> Output.E (10) [Exc]: Backprop\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "network_config.set_learning_rule_for_population(target_layer='H1', target_population='E', rule='Hebbian', learning_rate=0.01)\n",
    "network_config.print_architecture()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "70e18825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Network Architecture:\n",
      "Input.E (784) -> H1.E (500): Ojas_rule (lr=0.01)\n",
      "H1.SomaI (50) -> H1.E (500) [Inh]: Ojas_rule (lr=0.01)\n",
      "H1.E (500) -> H1.SomaI (50) [Exc]: Ojas_rule (lr=0.01)\n",
      "H1.E (500) -> Output.E (10) [Exc]: Backprop\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "network_config.set_learning_rule_for_layer(target_layer='H1', rule='Ojas_rule', learning_rate=0.01)\n",
    "network_config.print_architecture()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449c3973",
   "metadata": {},
   "source": [
    ":::{note}\n",
    "You need to make sure that the learning rule you want to use actually exists in the `EIANN.rules` module. In later tutorials, we will show you how to write your own learning rule.\n",
    ":::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db2c923",
   "metadata": {},
   "source": [
    "Once we have specified all the populations and projections that define the network architecture, we can use the `build_network` function to create the neural network (pytorch) object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3d26523",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network(\n",
      "  (criterion): MSELoss()\n",
      "  (module_dict): ModuleDict(\n",
      "    (H1E_InputE): Projection(in_features=784, out_features=500, bias=False)\n",
      "    (H1E_H1SomaI): Projection(in_features=50, out_features=500, bias=False)\n",
      "    (H1SomaI_H1E): Projection(in_features=500, out_features=50, bias=False)\n",
      "    (OutputE_H1E): Projection(in_features=500, out_features=10, bias=False)\n",
      "  )\n",
      "  (parameter_dict): ParameterDict(\n",
      "      (H1E_bias): Parameter containing: [torch.FloatTensor of size 500]\n",
      "      (H1SomaI_bias): Parameter containing: [torch.FloatTensor of size 50]\n",
      "      (OutputE_bias): Parameter containing: [torch.FloatTensor of size 10]\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "network = network_config.build()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e71121e",
   "metadata": {},
   "source": [
    "In the next tutorial, we will use EIANN to train a simple feedforward neural network to classify MNIST handwritten digits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f7f22d",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eiann",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
