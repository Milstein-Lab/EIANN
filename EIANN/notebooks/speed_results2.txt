Wrote profile results to forward_speedtest.py.lprof
Timer unit: 1e-06 s

Total time: 135.257 s
File: forward_speedtest.py
Function: forward at line 48

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
    48                                           @profile
    49                                           def forward(self, dataloader, store_history=False, store_dynamics=False, no_grad=True):
    50     60000    3908364.0     65.1      2.9      for idx, data, target in dataloader:
    51     60000     290572.0      4.8      0.2          sample = data.squeeze(0).to(device)
    52     60000      51092.0      0.9      0.0          if len(sample.shape) > 1:
    53                                                       batch_size = sample.shape[0]
    54                                                   else:
    55     60000       8641.0      0.1      0.0              batch_size = 1
    56    180000      98955.0      0.5      0.1          for i, layer in enumerate(self):
    57    120000      20242.0      0.2      0.0              if i == 0:
    58     60000      82396.0      1.4      0.1                  input_pop = next(iter(layer))
    59    300000     137175.0      0.5      0.1              for pop in layer:
    60    300000    2342891.0      7.8      1.7                  pop.reinit(self.device, batch_size=batch_size)
    61     60000     178678.0      3.0      0.1          input_pop.activity = torch.squeeze(sample)
    62                                           
    63    600000     160306.0      0.3      0.1          for t in range(self.forward_steps):
    64    600000     230018.0      0.4      0.2              if (t >= self.forward_steps - self.backward_steps) and not no_grad:
    65                                                           track_grad = True
    66                                                       else:
    67    600000      81053.0      0.1      0.1                  track_grad = False
    68    600000     813998.0      1.4      0.6              with torch.set_grad_enabled(track_grad):
    69   1800000     755424.0      0.4      0.6                  for post_layer in self:
    70   3000000    1256402.0      0.4      0.9                      for post_pop in post_layer:
    71   3000000    1754960.0      0.6      1.3                          post_pop.prev_activity = post_pop.activity
    72   1800000     869406.0      0.5      0.6                  for i, post_layer in enumerate(self):
    73   3000000    1463400.0      0.5      1.1                      for post_pop in post_layer:
    74   2400000     484821.0      0.2      0.4                          if i > 0:
    75   2400000   11658416.0      4.9      8.6                              delta_state = -post_pop.state + post_pop.bias
    76   3600000    3041536.0      0.8      2.2                              for projection in post_pop:
    77   3600000     838192.0      0.2      0.6                                  pre_pop = projection.pre
    78   3600000     850541.0      0.2      0.6                                  if projection.update_phase in ['forward', 'all', 'F', 'A']:
    79   2400000     511683.0      0.2      0.4                                      if projection.direction in ['forward', 'F']:
    80   2400000   53022066.0     22.1     39.2                                          delta_state = delta_state + projection(pre_pop.activity)
    81   1200000     292315.0      0.2      0.2                                      elif projection.direction in ['recurrent', 'R']:
    82   1200000   18082101.0     15.1     13.4                                          delta_state = delta_state + projection(pre_pop.prev_activity)
    83   2400000   22856762.0      9.5     16.9                              post_pop.state = post_pop.state + delta_state / post_pop.tau
    84   2400000    8540452.0      3.6      6.3                              post_pop.activity = post_pop.activation(post_pop.state)
    85   3000000     567146.0      0.2      0.4                          if store_dynamics:
    86                                                                       post_pop.forward_steps_activity.append(post_pop.activity.detach().clone())
    87                                           
    88     60000       6791.0      0.1      0.0          if store_history:
    89                                                       for layer in self:
    90                                                           for pop in layer:
    91                                                               pop.activity_history_list.append(pop.forward_steps_activity)

