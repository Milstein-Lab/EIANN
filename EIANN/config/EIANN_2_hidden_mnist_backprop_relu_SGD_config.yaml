layer_config:
  Input:
    E:
      size: 784
  H1:
    E:
      size: 500
      activation: relu
  H2:
    E:
      size: 500
      activation: relu
  Output:
    E:
      size: 10
      activation: relu


projection_config:
  H1:
    E:
      Input:
        E:
          direction: F
          learning_rule: Backprop
  H2:
    E:
      H1:
        E:
          direction: F
          learning_rule: Backprop
  Output:
    E:
      H2:
        E:
          direction: F
          learning_rule: Backprop


training_kwargs:
  tau: 1
  forward_steps: 1
  backward_steps: 1
  learning_rate: 0.01
  verbose: false
  optimizer: SGD
