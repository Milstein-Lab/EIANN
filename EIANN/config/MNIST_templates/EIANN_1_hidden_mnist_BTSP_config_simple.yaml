layer_config:
  Input:
    E:
      size: 784
  H1:
    E:
      size: 500
      activation: relu
  Output:
    E:
      size: 10
      activation: relu


projection_config:
  H1:
    E:
      Input:
        E:
          # weight_init: half_kaining
          direction: F
          learning_rule: almost_backprop

      # Output:
      #   E:
      #     weight_init: half_kaining
      #     update_phase: B
      #     compartment: dend
      #     learning_rule: null

  Output:
    E:
      H1:
        E:
          # weight_init: half_kaining
          direction: F
          learning_rule: almost_backprop


training_kwargs:
  tau: 1
  forward_steps: 1
  backward_steps: 1
  learning_rate: 0.01
