# Equivalent to dend_temp_contrast_fixed_bias from Summer2024 project

layer_config:
  Input:
    E:
      size: 2

  H1:
    E:
      size: 128
      activation: relu 
      bias_learning_rule: Null
      include_bias: True

  H2:
    E:
      size: 32
      activation: relu
      bias_learning_rule: Null
      include_bias: True

  Output:
    E:
      size: 4
      activation: relu


projection_config:
  H1.E:
    Input.E:
      direction: F
      learning_rule: BP_like_1E
      learning_rule_kwargs:
        max_pop_fraction: 1.0
        stochastic: false
        relu_gate: True
    H2.E:
      # direction: F
      update_phase: B
      learning_rule: null
      compartment: dend
      weight_constraint: clone_weight
      weight_constraint_kwargs:
        source: H2.E.H1.E
        transpose: true
  # above part says that H1.E gets projections from Input.E to the soma and H2.E to the dendrite

  H2.E:
    H1.E:
      direction: F
      learning_rule: BP_like_1E
      learning_rule_kwargs:
        max_pop_fraction: 1.0
        stochastic: false
        relu_gate: True
    Output.E:
      # direction: F
      update_phase: B
      learning_rule: null
      compartment: dend
      weight_constraint: clone_weight
      weight_constraint_kwargs:
        source: Output.E.H2.E
        transpose: true
        scale: 0.5

  Output.E:
    H2.E:
      direction: F
      learning_rule: BP_like_1E
      learning_rule_kwargs:
        max_pop_fraction: 1.0
        stochastic: false
        relu_gate: True

training_kwargs:
  tau: 1
  forward_steps: 1
  backward_steps: 0
  learning_rate: 0.06887924180457666